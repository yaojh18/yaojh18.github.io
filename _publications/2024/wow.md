---
title:          "Varying Shades of Wrong: Aligning LLMs with Wrong Answers Only"
date:           2024-10-14 00:00:00 +0000
selected:       true
pub:            "ICLR 2025"

abstract: >-
 What if we don't have high-quality preference data? We focus on the spectrum of wrongness and propose "wrong-over-wrong alignment", preferring less wrong answers over more wrong ones. Surprisingly, training on wrong answers only can guide models to produce correct answers.  
cover:          /assets/images/covers/wow.png
authors:
  - Jihan Yao*
  - Wenxuan Ding* 
  - Shangbin Feng*
  - Lucy Lu Wang
  - Yulia Tsvetkov
links:
  Code: https://github.com/yaojh18/Varying-Shades-of-Wrong
  Paper: https://arxiv.org/pdf/2410.11055
---
